{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup and Metrics",
   "id": "775b3e608ca4ec48"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-31T06:25:07.026343Z",
     "start_time": "2025-12-31T06:25:07.012081Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpmath as mp\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = True"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Floating-Point Types\n",
    "\n",
    "In 1985, the Institute of Electrical and Electronics Engineers (IEEE) introduced the **IEEE 754** standard, which defines how computers represent and perform arithmetic on real numbers using a finite number of binary bits.\n",
    "\n",
    "Most modern programming languages and hardware follow the IEEE 754 standard. For the purpose of this repository, Python and Jupyter notebooks are used, and all floating-point computations are performed using NumPy's IEEE-754 compliant data types: `np.float32` (single precision) and `np.float64` (double precision). These correspond directly to the standard 32-bit and 64-bit IEEE 754 floating-point formats and are used throughout the repository to study precision, rounding behavior, and numerical error.\n",
    "\n",
    "### Normalized Floating-Point Representation\n",
    "A (normalized) IEEE 754 floating-point number is represented as:  \n",
    "\n",
    "$(-1)^{sign} \\times 1.fraction \\times 2^{exponent}$  \n",
    "\n",
    "This mirrors scientific notation, but in base 2 instead of base 10.  \n",
    "\n",
    "#### Components of a Floating-Point Number\n",
    "**Sign Bit**  \n",
    "A single bit that determines the sign of the number:  \n",
    "`0` = Positive  \n",
    "`1` = Negative  \n",
    "\n",
    "**Mantissa (Fraction/Significand)**  \n",
    "Stores the significant digits of the number. For normalized numbers, the leading 1 is implicit.  \n",
    "\n",
    "**Exponent**  \n",
    "Stores the scale of the number using a bias so that both positive and negative exponents can be represented.  \n",
    "\n",
    "### Python (NumPy) Normalized Floating-Point Representation\n",
    "NumPy's `np.float32` and `np.float64` types follow the IEEE 754 standard for normalized floating-point numbers, differing only in the number of bits allocated to the exponent and mantissa.  \n",
    "\n",
    "`np.float32` (Single Precision)  \n",
    "A normalized `np.float32` value is stored using 32 bits:  \n",
    "Sign: 1 bit  \n",
    "Exponent: 8 bits (bias = 127)  \n",
    "Mantissa: 23 bits  \n",
    "Numerical Representation: $(-1)^{sign} \\times 1.fraction \\times 2^{exponent-127}$  \n",
    "\n",
    "`np.float64` (Double Precision)  \n",
    "A normalized `np.float64` value is stored using 64 bits:  \n",
    "Sign: 1 bit  \n",
    "Exponent: 11 bits (bias = 1023)  \n",
    "Mantissa: 52 bits  \n",
    "Numerical Representation: $(-1)^{sign} \\times 1.fraction \\times 2^{exponent-1023}$  "
   ],
   "id": "2c15b9729483b5f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T06:25:07.043354Z",
     "start_time": "2025-12-31T06:25:07.037784Z"
    }
   },
   "cell_type": "code",
   "source": "DTYPES = [np.float32, np.float64]",
   "id": "501a5a9a58d15e2e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Error Metrics\n",
    "The exact real-valued result of computations in floating-point arithmetic is rarely known due to the finite number of bits available in modern computers. Error metrics are used to compare computed **approximations** to a **reference** value that is assumed to be significantly more accurate.  \n",
    "\n",
    "There are two metrics that will be used: **Absolute Error** and **Relative Error**.  \n",
    "\n",
    "### Absolute Error\n",
    "The absolute error is defined as:  \n",
    "\n",
    "$Absolute\\;Error = |Computed\\;Value - Reference\\;Value|$  \n",
    "\n",
    "This metric measures the **raw** distance between the computed value and the reference value. Although, simple to implement and interpret the absolute error can be misleading for large magnitude values. An error of $10^{-6}$ may be good or bad depending on the scale of $x$. Because of this, the absolute error is best interpreted alongside relative error.\n",
    "\n",
    "### Relative Error\n",
    "The relative error is defined as:  \n",
    "\n",
    "$Relative\\;Error = \\frac{Absolute\\;Error}{|Reference\\;Value|}$  \n",
    "\n",
    "This metric measures the error **relative** to the magnitude of the reference value and indicates how many correct digits the approximation retains. A drawback of relative error is as the true value becomes close to 0, the metric becomes meaningless. Therefore, the relative error must always be interpreted in context."
   ],
   "id": "ac321abe3bf55ec4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T06:25:07.049040Z",
     "start_time": "2025-12-31T06:25:07.044101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def abs_error(true, approx):\n",
    "    return np.abs(true - approx)\n",
    "\n",
    "def rel_error(true, approx):\n",
    "    true = np.asarray(true)\n",
    "    approx = np.asarray(approx)\n",
    "    denom = np.maximum(np.abs(true), np.finfo(np.float64).tiny)\n",
    "    return np.abs(true - approx) / denom"
   ],
   "id": "86ea00780380ebec",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reference Value\n",
    "To quantify floating-point error, a value to compare against is needed. In most real problems the exact real-number result is unknown, so instead a **reference value** is computed and assumed to be significantly more accurate than the floating-point approximation being tested.  \n",
    "\n",
    "Here reference values are computed using **arbitrary-precision arithmetic** via `mpmath`. This will allow far more precision than `float32` (\\~7 decimal digits) and `float64` (\\~16 decimal digits), so we can meaningfully measure loss of significance, cancellation, and rounding error effects. "
   ],
   "id": "a83065b057a25f39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T06:25:07.054369Z",
     "start_time": "2025-12-31T06:25:07.049447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Decimal digits of precision\n",
    "mp.mp.dps = 80\n",
    "\n",
    "def mp_eval(func, x):\n",
    "    \"\"\"\n",
    "    Evaluate a function at a point x with high precision using mpmath.\n",
    "    :param func: A function that accepts an mp.mpf and returns an mp.mpf\n",
    "    :param x: Input value(s)\n",
    "    :return: np.ndarray or float\n",
    "    \"\"\"\n",
    "    xs = np.atleast_1d(x)\n",
    "    out = []\n",
    "    \n",
    "    for val in xs:\n",
    "        # Construct mpf from a string to avoid importing binary-float artifacts\n",
    "        out.append(func(mp.mpf(str(val))))\n",
    "    \n",
    "    out = np.array([float(v) for v in out], dtype=np.float64)\n",
    "    \n",
    "    return out if np.ndim(x) else out[0]"
   ],
   "id": "e84785836cfa0579",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example\n",
    "Below a high-precision reference for $(1-\\cos(x))$ is computed and compared against `float32` and `float64`."
   ],
   "id": "12ae60e9fd1e3888"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T06:25:07.066359Z",
     "start_time": "2025-12-31T06:25:07.054732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = 1e-12\n",
    "\n",
    "ref = mp_eval(lambda t: 1 - mp.cos(t), x)\n",
    "approx32 = np.float32(1 - np.cos(np.float32(x)))\n",
    "approx64 = np.float64(1 - np.cos(np.float64(x)))\n",
    "\n",
    "print(\"Reference: \", ref)\n",
    "print(\"Float32 Approximation: \", approx32)\n",
    "print(\"Float64 Approximation: \", approx64)\n",
    "print()\n",
    "print(\"Absolute Errors\")\n",
    "print(\"Float32: \", abs_error(ref, approx32))\n",
    "print(\"Float64: \", abs_error(ref, approx64))\n",
    "print()\n",
    "print(\"Relative Errors\")\n",
    "print(\"Float32: \", rel_error(ref, approx32))\n",
    "print(\"Float64: \", rel_error(ref, approx64))"
   ],
   "id": "478b688854024fd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:  5e-25\n",
      "Float32 Approximation:  0.0\n",
      "Float64 Approximation:  0.0\n",
      "\n",
      "Absolute Errors\n",
      "Float32:  5e-25\n",
      "Float64:  5e-25\n",
      "\n",
      "Relative Errors\n",
      "Float32:  1.0\n",
      "Float64:  1.0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Worked Example: Cancellation in $1 - \\cos(x)$\n",
    "\n",
    "The expression $1 - \\cos(x)$ is numerically unstable for small values of $x$. When $x \\to 0$, $\\cos(x) \\approx 1$, and subtracting two nearly equal numbers causes **catastrophic cancellation**, resulting in a severe loss of significant digits in floating-point arithmetic.  \n",
    "\n",
    "Although the exact value of $1 - \\cos(x)$ behaves like $\\mathcal{O}(x^2)$ for small $x$, both `float32` and `float64` arithmetic may round $\\cos(x)$ to exactly 1.0, causing the computed result to collapse to zero.  \n",
    "\n",
    "An algebraically equivalent but numerically stable formulation is given by the trigonometric identity:  \n",
    "\n",
    "$1 - \\cos(x) = 2\\sin^2\\!\\left(\\frac{x}{2}\\right)$  \n",
    "\n",
    "This reformulation avoids subtracting nearly equal quantities and instead computes small values directly, which is more robust in floating-point arithmetic.\n",
    "\n",
    "In the following example, we compare the unstable formulation $1 - \\cos(x)$ and the stable formulation $2\\sin^2(x/2)$ using both `float32` and `float64`, and measure their errors relative to a high-precision reference value computed using arbitrary-precision arithmetic."
   ],
   "id": "e3091b6dc9e24780"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T06:25:07.094650Z",
     "start_time": "2025-12-31T06:25:07.074115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = 1e-12\n",
    "\n",
    "ref = mp_eval(lambda t: 1 - mp.cos(t), x)\n",
    "\n",
    "unstable32 = np.float32(1 - np.cos(np.float32(x)))\n",
    "unstable64 = np.float64(1 - np.cos(np.float64(x)))\n",
    "\n",
    "stable32 = np.float32(2) * (np.sin(np.float32(x) / np.float32(2)) ** 2)\n",
    "stable64 = np.float64(2) * (np.sin(np.float64(x) / np.float64(2)) ** 2)\n",
    "\n",
    "print(\"Reference:\", ref)\n",
    "print()\n",
    "print(\"UNSTABLE: 1 - cos(x)\")\n",
    "print(\"float32:\", unstable32, \"abs:\", abs_error(ref, unstable32), \"rel:\", rel_error(ref, unstable32))\n",
    "print(\"float64:\", unstable64, \"abs:\", abs_error(ref, unstable64), \"rel:\", rel_error(ref, unstable64))\n",
    "print()\n",
    "print(\"STABLE: 2*sin(x/2)^2\")\n",
    "print(\"float32:\", stable32, \"abs:\", abs_error(ref, stable32), \"rel:\", rel_error(ref, stable32))\n",
    "print(\"float64:\", stable64, \"abs:\", abs_error(ref, stable64), \"rel:\", rel_error(ref, stable64))"
   ],
   "id": "f4b97f2f843cf579",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: 5e-25\n",
      "\n",
      "UNSTABLE: 1 - cos(x)\n",
      "float32: 0.0 abs: 5e-25 rel: 1.0\n",
      "float64: 0.0 abs: 5e-25 rel: 1.0\n",
      "\n",
      "STABLE: 2*sin(x/2)^2\n",
      "float32: 5e-25 abs: 9.770740727281028e-33 rel: 1.9541481454562058e-08\n",
      "float64: 5e-25 abs: 0.0 rel: 0.0\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Takeaway:** The direct computation $1-\\cos(x)$ suffers catastrophic cancellation for small $x$, causing both `float32` and `float64` to collapse to 0. Reformulating the expression using $1-\\cos(x)=2\\sin^2(x/2)$ avoids subtracting nearly equal numbers and restores accuracy, with `float32` achieving near its precision limit and `float64` matching the reference value in this test.",
   "id": "7f9e1a1494ccb51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "Wikipedia contributors. (2025, November 29). IEEE 754. In Wikipedia, The Free Encyclopedia. Retrieved 01:07, December 29, 2025, from https://en.wikipedia.org/w/index.php?title=IEEE_754&oldid=1324852683\n",
    "\n",
    "GeeksforGeeks. (2024, August 5). Absolute error and relative error: Formula and equation. https://www.geeksforgeeks.org/maths/absolute-error-and-relative-error-formula-and-equation/ \n",
    "\n",
    "Wikipedia contributors. (2025, February 16). Mean absolute error. In Wikipedia, The Free Encyclopedia. Retrieved 02:05, December 29, 2025, from https://en.wikipedia.org/w/index.php?title=Mean_absolute_error&oldid=1276071917\n",
    "\n",
    "Wikipedia contributors. (2025, November 18). Approximation error. In Wikipedia, The Free Encyclopedia. Retrieved 02:06, December 29, 2025, from https://en.wikipedia.org/w/index.php?title=Approximation_error&oldid=1322965821"
   ],
   "id": "fab39e197bcfaa17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
